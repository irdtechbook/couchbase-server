
= アーキテクチャー

== 分散アーキテクチャー

分散アーキテクチャーでは、複数のサーバを用いて、一つのプラットフォームを構成します。これを一般に@<strong>{クラスター}と呼びます。また、クラスターを構成する各サーバーは@<strong>{ノード}と呼ばれます。
クラスターは、ノード(物理サーバー、仮想マシン、あるいはコンテナ)の追加により、CPU、RAM、ディスク、およびネットワークの容量を増やすことができます。

Couchbase Serverは、シェアードナッシング型の@<strong>{分散アーキテクチャー}を特徴としており、拡張性と可用性に優れています。

=== ピアツーピアトポロジー

Couchbase Serverにおいては、他の分散システムに見られるような、マスターノード、スレーブノード、構成ノード、名前ノード、ヘッドノードなどの概念はなく、@<strong>{全てのノードにおいてソフトウェアは同一}です。
すべてのノードは同等のレイヤーで相互に通信します（@<strong>{ピアツーピアトポロジー}）。
Couchbase Serverは通常、複数のコモディティサーバーから構成されるクラスターとしてデプロイされますが、開発などの目的で、すべての機能を単一のノードで実行することもできます。
その場合であっても、他の分散アーキテクチャーに見られるような（例えばスタンドアローンモードのような）、通常とは異なる特別なデプロイ形態は存在しません。
単一のノードで構成した場合も、複数のノードで構成した場合も、本質的なアーキテクチャーや挙動は変わるところがありません。

Couchbase Serverでは、ノードがクラスターの一部として構成される（クラスターに参加する）際に、そのノードの特定のサービスを有効または無効にできます(Couchbase Server管理者は、クラスターへノードを追加する際に、そのノードで実行されるサービスを選択します)。
ただし、一旦そのノードがクラスターに参加した後は、そのノードで動いているサービスを変更することはできません。そのようなことを行いたい場合には、一度そのノードをクラスターのメンバーから削除し、改めて参加させる際にサービスを選択し直します。

=== ピアツーピアレプリケーション

CouchbaseServerはアクティブなデータのコピー（レプリカ）を自動的に作成し、それらのレプリカをクラスター内のノードに分散して、すべてのコピーが別々のノードに配置されるようにします。
これは、他のデータベースに見られるように、サーバーのレベルで、プライマリーサーバーとセカンダリーサーバーという役割の違うノードから構成されるアーキテクチャーとは異なっています。
Couchbase Serverのこのような特徴は、@<strong>{ピアツーピアレプリケーション}と呼ばれます。
Couchbase Serverは、最大3つのレプリカをサポートします。ピアツーピアレプリケーションは、アクティブデータのコピーを他の（複数の）ノードに複製するため、単一障害点を排除します。

ノードがダウンした場合、Couchbase Serverは、クラスター内の他の場所に存在するレプリカをアクティブ化することによって、そのデータを回復します。このプロセスはフェイルオーバーと呼ばれます。フェイルオーバーは自動または手動で行うことができます。

レプリケーションが提供する冗長性は、単一ノード上のデータの損失から保護し、ハードウェア障害やサービスの中断からの回復を可能にすることでデータの可用性を向上させるのに役立ちます。

レプリカは高可用性のみを目的としており、アクティブデータへの昇格が行われない限り、通常は使用されません。これにより、分散環境に起因する、データの一貫性を維持するためのオーバーヘッドが取り除かれ、非常に高いスループットと低遅延が可能になります。1つのノードに対する1秒あたり数十万の要求の99パーセンタイルで、1ミリ秒未満の応答時間を測定することは珍しくありません。

一方、レプリカへのアクセスのためのAPIも提供されており、アクティブデータが利用できなくなってからフェイルオーバーが実行されるまでの期間、アプリケーションからそのAPIを用いて、読み取りの可用性を向上させることができます。

//image[replica-concept][]{
//}

(画像は、 Couchbase Under the Hood: An Architectural Overview@<fn>{server-arc-overview}より引用)

====[column]レプリカの数え方
分散アーキテクチャーを持つソフトウェアの中には、例えば３レプリカと言った場合、そのソフトウェアがデータのコピーを3つ持っていることを意味するものがあります(例えばHadoop)。
Couchbase Serverでは、3レプリカと言った場合、アクティブデータを含めて、4つのコピーが存在しています。
この違いは、前者の種類のソフトウェアでは、複製されたデータが区別なく扱われるのに対して、Couchbase Serverは、アクティブデータとレプリカデータが区別されるアーキテクチャーであることから生じています。

====[/column]


=== マルチディメンショナルスケーリング(MDS)

ノードは、ドキュメントストレージ（Dataサービス）、クエリ（Queryサービス）、インデックス（Indexサービス）等、特定のワークロードの処理専用に構成することができます。これによってワークロードを分離し、各サービスの要件と特性に応じたリソース配分にてクラスターを構成することができます。
他の分散アーキテクチャーを持つソフトウェアでは、ノードは全て同一のリソースを持つハードウェアで構成されることが必要ないし前提とされることが見られるのに対して、Couchbase Serverは、リソース配分の柔軟性を特徴としており、ノード数(水平方向)だけでなく、各ノードでワークロード/サービスの特性に応じた異なるリソース（メモリ、CPU）を用いることができ(垂直方向)、こうした水平および垂直に構成可能な柔軟なスケーリングは、@<strong>{マルチディメンショナルスケーリング(MDS)}と呼び習わされています。

====[column]エディションによる差異
MDSは、エンタープライズエディションのみサポートされています。具体的には、コミュニティエディションでは、ノード上のサービスとしてDataサービスを無効にすることができません。つまり、エンタープライズエディションでは、Queryサービス等を専用のノードを用いて運用することができますが、コミュニティエディションでは、Dataサービス以外のサービスは、常にDataサービスと同じハードウェアリソースを共有することになります。
また、コミュニティエディションではクエリの並列実行におけるリソース利用は、最大４コアに制限されているのに対して、エンタープライズエディションではこの制約はない、と言った違いが見られます。

====[/column]

=== オンラインリバランス

ノードの追加と削除は、データを均等に再分散するリバランスプロセスを通じて、実現されます。
リバランスプロセスはオンラインで行われ、リバランス中もクライアントはCouchbase Serverへ継続してアクセスすることができます。


=== データベースチェンジプロトコル(DCP)

Couchbase Serverの各ノードは、データの複製、インデックスの更新などのために相互に通信します。
その際、Couchbase Serverは、固有のストリーミングプロトコルである@<strong>{データベースチェンジプロトコル(DCP)}を用います。DCPは、Couchbase Serverで生じたデータの変更をクラスター内部に伝える役割を持ちます。
また、DCPは、Spark、Kafka、Elasticsearchなどの外部システムとの統合のための@<strong>{コネクター}へデータをフィードするためにも用いられます。

DCPにおけるデータ転送は、メモリーのレイヤーで実行されます。つまり、Dataサービスで、あるデータの変更がメモリレベルで行われた後、ディスクへの反映を待たずに、DCPを介して他のノードへ転送されます。
DCPは、一時的なエラーに対して堅牢で回復力を持ちます。例えばストリームが中断された場合、接続が再開されるとDCPは最後に正常に更新された時点から再開します。

=== vBucket

Couchbase Serverは内部的に、@<strong>{vBucket}を使用して、データを管理しています（クライアント/アプリケーションはvBucketを直接操作しません）。
シャードまたはパーティションのようなものといえば、これらの用語に慣れている方はにとっては、理解しやすいかもしれません。あるいは、もっと直接的に、OS上の物理ファイルに対応しているともいえます。

一つの(論理)バケットは、1024個のアクティブvBucketと、（１レプリカに対して）1024個のレプリカvBucketに分割して管理されています。この数はクラスターを構成するノードの数に左右されません。
vBucketは、クラスター内でDataサービスを実行しているノード全体に均等に分散されます。Dataサービスを実行しているノードの数に増減が生じた場合には、vBucketは再分配されます。


== スマートクライアント

=== クラスターマップ

Couchbase Serverのクライアントは、接続されているCouchbase Serverクラスターのトポロジー(クラスターの構成)を認識し、トポロジーに変更が発生しても、透過的に最新の状態を認識します。
これは、@<strong>{クラスターマップ}と呼ばれる内部情報が、クラスターから、クライアントへ提供されることによって実現されます。
クライアントは、クラスターへの初回接続（@<strong>{ブートストラップ}）時にクラスターマップを入手し、クライアントとクラスターとの接続中、クラスターマップは最新の状態に維持されます。このような機構は、@<strong>{スマートクライアント}と呼ばれています。

Dataサービスと対話する際、クライアントは、任意のドキュメントを管理しているノードに直接アクセスします。これは、Couchbase Serverからのトポロジ変更通知に基づいて、クラスターマップのローカルコピーを保存し、透過的に更新することによって機能します。

Dataサービスにおいて、データ（ドキュメント）は、格納されているバケットにおいて一意のキー（ドキュメントキー、またはドキュメントID）を持ちます。キーは、クライアント/アプリケーションにより自由につけることができますが、内部的には、CRC32ハッシュアルゴリズムを用いて変換されています。このアルゴリズムにより、バケットを構成するvBucket中のデータ（ドキュメント）の分布は、キーの内容に関わらず、均等になります

Couchbase Serverクライアントは、ドキュメントのアクティブなコピーを見つけるために、ドキュメントのキーに対して、CRC32ハッシュアルゴリズムを使用して、どのvBucketがそのキーを担当しているかを識別します。次に、クライアントはクラスターマップを参照して、現在どのノードに、そのvBucketが含まれているかを判別します。

同様のトポロジ認識を使用して、Queryサービスを実行しているノードのリストが維持されます。Queryサービスはステートレスであるため、任意のノードで任意のリクエストを処理できます。

//image[cluster-map][]{
//}

(画像は、 Couchbase Under the Hood: An Architectural Overview@<fn>{server-arc-overview}より引用)

スマートクライアントによるトポロジーの認識は、他の分散アーキテクチャーを持つデータプラットフォームには見られない、Couchbase Serverに独自な部分です。
多くの分散アーキテクチャーを持つデータベースでは、クライアントとのコミュニケーションを担当する特別なノードが存在しています。
この違いは、Couchbase Serverが、クライアント/アプリケーションに対して低遅延かつハイスループットな処理を提供することができる要因の１つです。


=== ブートストラップ

クライアントからCouchbase Serverへの初回接続は、認証と認可(承認)、検出(Discovery)、およびサービス接続の3つのフェーズで確立されます。この一連の過程は、@<strong>{ブートストラップ}と呼ばれます。

 1. @<strong>{認証と認可} はじめに、クライアントは、ユーザー名とパスワードで認証されます。認証を介して、ユーザーはロールを取得します。ロールに含まれる権限によって、Couchbase Serverのリソースに対する操作が認可されます。
 2. @<strong>{検出(Discovery)} クラスターマップがクライアントに返されます。これは、現在のクラスタートポロジを示しています。クラスターマップは、クラスターを構成するノードのリスト、ノード間のサービス配置、およびノード間のデータ配置に関する情報を含みます。クライアントは、これらの情報を持っているため、適切なノードへアクセスすることができます。これは、単に適切なサービスを提供するノードへアクセスできることを意味しているだけなく、データの作成や（検索クエリではなく、キー/IDによる）取得の際に、そのデータの管理を担当するノードへ直接アクセスができることを意味しています。
 3. @<strong>{サービス接続} クラスターマップを取得すると、クライアントはサービスレベルの操作を実行するために必要な接続を確立します。この時実行しようとする操作の種類・内容によって、認可が必要になる場合があります。クライアントが要求しているリソースへのアクセスに対する適切な権限に関連付けられているロールを持っている場合、アクセスが許可されます。
 
なお、ブートストラップ時の接続先は、高可用性を実現するために、クラスターを構成する複数のノードのIPアドレスを用いて構成するのが一般的です。

== XDCR(Xross Data Center Replication)

====[column]エディションによる差異
コミュニティエディションでは、XDCRは利用できません(バージョン7.0より。バージョン6.6までは、一部機能を除き利用できます)。

====[/column]

=== XDCRは、何故必要か？

XDCR(クロスデータセンターレプリケーション)は、Couchbase　Serverクラスター間でデータを複製するために使用されるテクノロジーです。
XDCRを使用すると、ベアメタル、VM、プライベートクラウド、パブリッククラウド、ハイブリッド、コンテナなどのプラットフォームに関係なく、任意のCouchbase Serverクラスター間でデータを複製できます。

=== XDCRは、どのように動作するか？

XDCRは非同期レプリケーションによる結果整合性を介して、サイト間でデータの一貫性を維持します。片方向のデータ同期のみではなく、双方向のデータ同期をサポートしています。

XDCRは、クラスター内部の構成変更に対して透過的に動作します。計画的ないし障害によるノードの追加・削除（トポロジー変更）中であっても、マニュアルの対応は不要であり、継続して動作し続けます。

Couchbase　Serverは、「データベース変更プロトコル（DCP）」と呼ばれるストリーミングプロトコルを持ち、順序付けられたキューを使用してデータの変更を通信します。XDCRはDCPのコンシューマーであり、DCPに依存しています。

//image[XDCR][]{
//}

(画像は、 Couchbase Under the Hood: An Architectural Overview@<fn>{server-arc-overview}より引用)

=== チェックポイント

XDCRは、レプリケーション実行中、一定の間隔で、チェックポイントを更新します。レプリケーション中断後の再開時(これには、ユーザーによる計画的な実行、または障害からの回復のケースが考えられます)、XDCRは最後のチェックポイントからレプリケーションを再開します。。

チェックポイント更新の間隔が短いほど、チェックポイントの精度が高くなり、レプリケーションの再開に必要な作業量が少なくなります。一方、頻繁なチェックポイントの更新はシステムリソースを使用し、パフォーマンスに影響を与える可能性があります。

=== レプリケーションフィルター

レプリケーションの対象とするデータを、ユーザがデータの内容などに応じてフィルタリングするための方法（レプリケーションフィルター）が用意されています。

====[column]エディションによる差異
コミュニティエディションでは、(XDCRが利用できるバージョンであっても)フィルタリング機能は利用できません。

====[/column]


=== レプリケーション優先度設定

（あるバケットについて）レプリケーションが実行されている環境で、新しく（別のバケットのために）新規レプリケーションを追加するようなケースでは、新規レプリケーションの実行が、既存のレプリケーションのスループットに悪影響を与える場合が考えられます（例えば、既存レプリケーションでは差分データ更新が行われているのに対して、新規レプリケーションでは全件データのレプリケーションが一斉に実行されることにより）。XDCRでは、レプリケーションに対して優先度を設定することができ、このようなケースにおいても、優先度に基づいて適切なリソース配分を実現することが可能です。

=== ネットワーク帯域幅節約のための最適化

XDCRは、データ圧縮のような内部機構や、ユーザーによる設定（例えば、高度な設定として提供されている「オプティミスティックレプリケーション」）を介して、ネットワークの帯域幅を節約するために高度に最適化されています。

=== 運用上の注意

XDCR処理は、Dataサービスが稼働しているノードで実行されるため、多かれ少なかれDataサービスに対する性能影響があります。適切にリソース見積もりが行われることが重要ですが、特に既存のサービス運用中のクラスターで、新たにXDCRを利用する要件が生まれた際には注意が必要です。典型的なケースとして、データセンターのマイグレーションが考えられます。状況に応じて、マイグレーションのためにデータのレプリケーションを行う期間、一時的にノードを追加し、クラスターを拡張するといった対応も考えられます。

=== 参考情報

Couchbase公式ドキュメント XDCR Advanced Settings@<fn>{xdcr-advanced-settings}

Couchbaseブログ Understanding Cross Datacenter Replication (XDCR) -- Part 1@<fn>{understanding-xdcr-part-1}

//footnote[server-arc-overview][https://resources.couchbase.com/c/server-arc-overview?x=V3nd_e]

//footnote[xdcr-advanced-settings][https://docs.couchbase.com/server/current/xdcr-reference/xdcr-advanced-settings.html]

//footnote[understanding-xdcr-part-1][https://blog.couchbase.com/understanding-xdcr-part-1/]
