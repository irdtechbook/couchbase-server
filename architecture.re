
= アーキテクチャー

== 分散システムアーキテクチャー

=== 分散アーキテクチャーの特徴

Couchbase
Serverは、シェアードナッシング型の@<strong>{分散アーキテクチャー}を特徴としており、拡張性・可用性に優れています。

分散アーキテクチャは、複数のサーバを用いて、一つのプラットフォームを構成します。これを一般に@<strong>{クラスター}と呼びます。また、クラスタを構成する各サーバは@<strong>{ノード}と呼ばれます。Couchbase
Serverは、そのトポロジー（ノード構成）に関わらず、クラスターとして柔軟に構成可能です。

Couchbase Serverにおいては、他の分散システムに見られるような、マスターノード、スレーブノード、構成ノード、名前ノード、ヘッドノードなどの概念はなく、@<strong>{全てのノードにおいてソフトウェアは同一}です。その意味で、すべてのノードは同等のレイヤーで相互に通信します（ピアツーピアトポロジー）。Couchbase
Serverは通常、複数のコモディティサーバーから構成されるクラスターとしてデプロイされますが、開発などの目的で、すべての機能を単一のノードで実行することもでき、その場合にもアーキテクチャーや挙動については変わるところがありません(スタンドアローンモードのような通常とは異なる特別のデプロイ形態は存在しません)。

=== クラスターの構成変更

クラスターは、物理サーバー、仮想マシン、あるいはコンテナの追加により、CPU、RAM、ディスク、およびネットワークの容量を増やすことができます。

ノードが追加される際に、ユーザーは、そのノードで実行されるサービスを構成します。ユーザーは、あるノードがクラスターの一部としてはじめに構成される（クラスターに参加する）際に、そのノードで特定のサービスを有効または無効にできます。

====[column]
既にクラスターを構成しているノードのサービスを変更することはできません。そのようなことを実現したい場合には、一度そのノードをクラスターのメンバーから除いた後に、改めて参加させる際にサービスを選択します。

====[/column]

ノードは、簡単に追加または削除できます。Dataサービスノードの追加・削除は、すべてのノードにデータを均等に再分散するリバランスプロセスを通じて、実現されます。リバランスプロセスはオンラインで行われ、アプリケーションのダウンタイムは必要ありません。Queryサービスは、ステートレスであり、クラスター構成変更の影響を受けません。Indexサービスが管理するインデックスのノードへの配置は、ユーザーによりカスタマイズすることが可能であるため、クラスター構成変更時の影響関係がユーザーによって把握されている必要があります。

=== ワークロード分散

ノードは、特定のワークロード〜ドキュメントストレージ（Dataサービス）、クエリ（Queryサービス）、インデックス（Indexサービス）等〜の処理専用に構成することによってワークロードを分離し、クラスターを各サービスの要件・特性に応じたリソース配分にて構成することができます。このリソース配分には、サービス当たりのノード数のみではなく、各ノードでワークロード/サービスの特性に応じたリソース（メモリ、CPU）を割り当てることも含まれ、こうした水平および垂直に構成可能なスケーリングは、@<strong>{マルチ・ディメンショナル・スケーリング(MDS)}と呼び習わされています。

====[column]
MDSは、エンタープライズエディションのみサポートされるとされています。具体的には、コミュニティエディションでは、ノード上のサービスとしてDataサービスを無効にすることができません。つまり、エンタープライズエディションでは、Queryサービス等をそのサー ビス専用のノードを用いて運用することができますが、コミュニティエディションでは、Dataサービス以外のサービスは、常にDataサービスと同じハードウェアリソースを共有することになります。これに付随して、コミュニティエディションではクエリの並列実行におけるリソース利用は、Dataサービスとの共存が前提であることから、最大４コアに制限されているのに対して、エンタープライズエディションではこの制約はない、と言った違いが見られます。

====[/column]

=== レプリケーション

CouchbaseServerはアクティブなデータのコピー（レプリカ）を自動的に作成し、それらのレプリカをクラスター内のノードに分散して、すべてのコピーが別々のノードに配置されるようにします（ピアツーピアレプリケーション）。Couchbase
Serverは、最大3つのレプリカ（つまり、最大4つのデータコピー）をサポートします。ピアツーピアレプリケーションは、アクティブデータのコピーを他の（複数の）ノードに複製するため、単一障害点を排除します。

ノードがダウンした場合、Couchbase Serverは、クラスター内の他の場所に存在するレプリカをアクティブ化することによって、そのデータを回復します。このプロセスはフェイルオーバーと呼ばれます。フェイルオーバーは自動または手動で行うことができます。

レプリケーションが提供する冗長性は、単一ノード上のデータの損失から保護し、ハードウェア障害やサービスの中断からの回復を可能にすることでデータの可用性を向上させるのに役立ちます。

レプリカは高可用性のみを目的としており、アクティブデータへの昇格が行われない限り、通常は使用されません。これにより、分散環境に起因する、データの一貫性を維持するためのオーバーヘッドが取り除かれ、非常に高いスループットと低遅延が可能になります。1つのノードに対する1秒あたり数十万の要求の99パーセンタイルで、1ミリ秒未満の応答時間を測定することは珍しくありません。

一方、レプリカへのアクセスのためのAPIも提供されており、アクティブコピーが利用できなくなってからフェイルオーバーが実行されるまでの期間、アプリケーションからそのAPIを用いて、読み取りの可用性を向上させることができます。

//image[replica-concept][replica concept]{
//}

(画像は、 Couchbase Under the Hood: An Architectural Overview@<fn>{server-arc-overview}より引用)

=== データの分散管理（シャード）

Couchbase Serverは、内部的に、@<strong>{vBucket}（シャードまたはパーティションと同義）を使用して、データを管理しています（ユーザー/アプリケーションはvBucketを直接操作しません）。一つのバケットは、1024個のアクティブvBucketと、（１レプリカに対して）1024個のレプリカvBucketに分割して管理されています。vBucketは、クラスター内でDataサービスを実行しているノード全体に均等に分散されます。Dataサービスを実行しているノードの数に増減が生じた場合には、vBucketは再分配されます。

=== スマートクライアント

Couchbase Serverのクライアントは、接続されているCouchbase Serverクラスターのトポロジを自動的に認識し、トポロジーに変更が発生しても、変更を透過的に維持します。これは、@<strong>{クラスターマップ}と呼ばれる内部情報が、クラスターから、クライアントへ提供されることによって実現されます。クライアントは、クラスターへの初回接続（ブートストラップ）時にクラスターマップを入手し、クライアントとクラスターとの接続中、クラスターマップは最新の状態に維持されます。このような機構は、@<strong>{スマートクライアント}と呼ばれています。
なお、ブートストラップ時の接続先は、高可用性を実現するために、クラスターを構成する複数のノードのIPアドレスを用いて構成するのが一般的です。


Dataサービスと対話する際、クライアントは、何らかのプロキシやルーティングエンティティに問い合わせることなく、任意のドキュメントを管理しているノードに直接アクセスします。これは、Couchbase Serverからのトポロジ変更通知に基づいて、クラスターマップのローカルコピーを保存し、透過的に更新することによって、機能します。

Dataサービスにおいて、データ（ドキュメント）は、格納されているバケットにおいて一意のキー（ドキュメントキー、またはドキュメントID）を持ちます。キーは、ユーザー/アプリケーションにより自由につけることができますが、内部的には、CRC32ハッシュアルゴリズムを用いて変換されています。このアルゴリズムにより、バケットを構成するvBucket中のデータ（ドキュメント）の分布は、キーの内容に関わらず、均等になります

Couchbase Serverクライアントは、ドキュメントのアクティブなコピーを見つけるために、ドキュメントのキーに対して、CRC32ハッシュアルゴリズムを使用して、どのvBucketがそのキーを担当しているかを識別します。次に、クライアントはクラスターマップを参照して、現在どのノードに、そのvBucketが含まれているかを判別します。

同様のトポロジ認識を使用して、Queryサービスを実行しているノードのリストが維持されます。Queryサービスはステートレスであるため、任意のノードで任意のリクエストを処理できます。

//image[cluster-map][cluster map]{
//}

(画像は、 Couchbase Under the Hood: An Architectural Overview@<fn>{server-arc-overview}より引用)

=== データ転送

Couchbase Serverは、その内部で固有のストリーミングプロトコル、@<strong>{データベース・チェンジ・プロトコル(DCP)}を用います。DCPは、クラスター内部、および、クラスター外部に対して、Couchbase　Serverで生じたデータの変更を伝える役割を持ちます。

Couchbase Server内では、DCPを介して、レプリカ、インデックスなどを更新します。また、DCPは、Spark、Kafka、Elasticsearchなどの外部システムとの統合のためのコネクターへデータをフィードするためにも用いられます。また、他のCouchbase
Serverクラスターとのデータ同期のためにも用いられます。

DCPにおけるデータ転送は、メモリーのレイヤーで実行されます。つまり、Dataサービスで、あるデータの変更がメモリレベルで行われた後、ディスクへの反映を待たずに、DCPを介して他のノードへ転送されます。

DCPは、一時的なエラーに対して堅牢で回復力を持ちます。例えばストリームが中断された場合、接続が再開されるとDCPは最後に正常に更新された時点から再開します。

== XDCR(Xross Data Center Replication)

====[column] エディションによる差異
コミュニティエディションでは、XDCRは利用できません(バージョン7.0より。バージョン6.6までは、一部機能を除き利用できます)。

====[/column]

=== XDCRは、何故必要か？

XDCR(クロスデータセンターレプリケーション)は、Couchbase　Serverクラスター間でデータを複製するために使用されるテクノロジーです。
XDCRを使用すると、ベアメタル、VM、プライベートクラウド、パブリッククラウド、ハイブリッド、コンテナなどのプラットフォームに関係なく、任意のCouchbase Serverクラスター間でデータを複製できます。

=== XDCRは、どのように動作するか？

XDCRは非同期レプリケーションによる結果整合性を介して、サイト間でデータの一貫性を維持します。片方向のデータ同期のみではなく、双方向のデータ同期をサポートしています。

XDCRは、クラスター内部の構成変更に対して透過的に動作します。計画的ないし障害によるノードの追加・削除（トポロジー変更）中であっても、マニュアルの対応は不要であり、継続して動作し続けます。

Couchbase　Serverは、「データベース変更プロトコル（DCP）」と呼ばれるストリーミングプロトコルを持ち、順序付けられたキューを使用してデータの変更を通信します。XDCRはDCPのコンシューマーであり、DCPに依存しています。

//image[XDCR][XDCR]{
//}

(画像は、 Couchbase Under the Hood: An Architectural Overview@<fn>{server-arc-overview}より引用)

=== 機能ハイライト

==== チェックポイントによるレプリケーション中断・再実行制御

XDCRは、レプリケーション実行中、一定の間隔で、チェックポイントを更新します。レプリケーションの中断後、XDCRは最後のチェックポイントからレプリケーションを再開します。これには、ユーザーによる計画的な実行、または障害からの回復のケースが考えられます。

チェックポイント更新の間隔が短いほど、チェックポイントの精度が高くなり、レプリケーションの再開に必要な作業量が少なくなります。一方、頻繁なチェックポイントの更新はシステムリソースを使用し、パフォーマンスに影響を与える可能性があります。

==== 複数のレプリケーション間の優先度設定

（あるバケットについて）レプリケーションが実行されている環境で、新しく（別のバケットのために）新規レプリケーションを追加するようなケースでは、新規レプリケーションの実行が、既存のレプリケーションのスループットに悪影響を与える場合が考えられます（例えば、既存レプリケーションでは差分データ更新が行われているのに対して、新規レプリケーションでは全件データのレプリケーションが一斉に実行されることにより）。XDCRでは、レプリケーションに対して優先度を設定することができ、このようなケースにおいても、優先度に基づいて適切なリソース配分を実現することが可能です。

==== ネットワーク帯域幅節約のための最適化

XDCRは、データ圧縮のような内部機構や、ユーザーにより設定（例えば、高度な設定として提供されている「オプティミスティックレプリケーション」）を介して、ネットワークの帯域幅を節約するために高度に最適化されています。

==== レプリケーション対象データのフィルタリング

Couchbase Serverのエンタープライズエディションでは、レプリケーションの対象とするデータを、ユーザがデータの内容などに応じてフィルタリングするための方法（レプリケーションフィルター）が用意されています。

====[column] エディション間の差異
コミュニティエディションでは、(XDCRが利用できるバージョンであっても)フィルタリング機能は利用できません。

====[/column]

=== 運用上の注意

XDCR処理は、Dataサービスが稼働しているノードで実行されるため、多かれ少なかれDataサービスに対する性能影響があります。適切にリソース見積もりが行われることが重要ですが、特に既存のサービス運用中のクラスターで、新たにXDCRを利用する要件が生まれた際には注意が必要です。典型的なケースとして、データセンターのマイグレーションが考えられます。状況に応じて、マイグレーションのためにデータのレプリケーションを行う期間、一時的にノードを追加し、クラスターを拡張するといった対応も考えられます。

==== 参考情報

Couchbase公式ドキュメント XDCR Advanced Settings@<fn>{xdcr-advanced-settings}

Couchbaseブログ Understanding Cross Datacenter Replication (XDCR) -- Part 1@<fn>{understanding-xdcr-part-1}

//footnote[server-arc-overview][https://resources.couchbase.com/c/server-arc-overview?x=V3nd_e]

//footnote[xdcr-advanced-settings][https://docs.couchbase.com/server/current/xdcr-reference/xdcr-advanced-settings.html]

//footnote[understanding-xdcr-part-1][https://blog.couchbase.com/understanding-xdcr-part-1/]
